import collections
import json
import numpy as np
from pathlib import Path
import torch
from transformers import (
    BertTokenizerFast,
    BertForQuestionAnswering,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
)

MODEL_NAME = "bert-base-chinese"


def load_data(path):
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    return data


def load_context(context_path):
    """è¼‰å…¥æ®µè½é™£åˆ—ï¼Œç´¢å¼•å³ç‚ºæ®µè½ ID"""
    with open(context_path, encoding="utf-8") as f:
        contexts = json.load(f)
    return contexts


def prepare_train_features(examples, contexts, tokenizer, max_length=512, doc_stride=128):
    """è™•ç†è¨“ç·´è³‡æ–™"""
    all_encodings = {
        "input_ids": [],
        "attention_mask": [],
        "start_positions": [],
        "end_positions": [],
    }
    
    skipped = 0
    total_features = 0
    
    for example in examples:
        question = example["question"]
        relevant_id = example["relevant"]
        answer = example["answers"][0]
        answer_text = answer["text"]
        answer_start = answer["start"]
        
        # å–å¾—ç›¸é—œæ®µè½ï¼ˆç›´æ¥ç”¨ ID ä½œç‚ºç´¢å¼•ï¼‰
        if relevant_id >= len(contexts):
            print(f"Warning: Context ID {relevant_id} out of range")
            skipped += 1
            continue
        
        context = contexts[relevant_id]
        
        # é©—è­‰ç­”æ¡ˆåœ¨æ®µè½ä¸­
        if answer_text not in context:
            print(f"Warning: Answer '{answer_text}' not in context {relevant_id}")
            skipped += 1
            continue
        
        # é©—è­‰ answer_start ä½ç½®
        if context[answer_start:answer_start + len(answer_text)] != answer_text:
            # å¦‚æœ start ä½ç½®ä¸å°ï¼Œç”¨æ–‡å­—æœå°‹
            try:
                answer_start = context.index(answer_text)
            except ValueError:
                print(f"Warning: Cannot find answer '{answer_text}' in context {relevant_id}")
                skipped += 1
                continue
        
        answer_end = answer_start + len(answer_text)
        
        # â­ Sliding window tokenize
        tokenized = tokenizer(
            question,
            context,
            truncation="only_second",
            max_length=max_length,
            stride=doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )

        for i in range(len(tokenized["input_ids"])):
            input_ids = tokenized["input_ids"][i]
            attention_mask = tokenized["attention_mask"][i]
            offsets = tokenized["offset_mapping"][i]
            sequence_ids = tokenized.sequence_ids(i)

            # æ‰¾ CLS indexï¼ˆé€šå¸¸æ˜¯ 0ï¼‰
            cls_index = input_ids.index(tokenizer.cls_token_id)

            # æ‰¾é€™å€‹ feature çš„ context token ç¯„åœ
            context_start = 0
            while context_start < len(sequence_ids) and sequence_ids[context_start] != 1:
                context_start += 1

            context_end = len(sequence_ids) - 1
            while context_end >= 0 and sequence_ids[context_end] != 1:
                context_end -= 1

            # è‹¥é€™å€‹ window æ²’æœ‰ context
            if context_start >= len(sequence_ids) or context_end < 0:
                all_encodings["input_ids"].append(input_ids)
                all_encodings["attention_mask"].append(attention_mask)
                all_encodings["start_positions"].append(cls_index)
                all_encodings["end_positions"].append(cls_index)
                total_features += 1
                continue

            # window çš„ char ç¯„åœï¼ˆåªçœ‹ context éƒ¨åˆ†ï¼‰
            # offsets åœ¨ question/special çš„ token å¯èƒ½æ˜¯ (0,0) æˆ– None
            # æˆ‘å€‘åªä¿¡ sequence_ids==1 çš„ offsets
            start_char = offsets[context_start][0]
            end_char = offsets[context_end][1]

            # è‹¥ç­”æ¡ˆå®Œå…¨ä¸åœ¨æ­¤ windowï¼Œæ¨™ CLS
            if not (start_char <= answer_start and answer_end <= end_char):
                # all_encodings["input_ids"].append(input_ids)
                # all_encodings["attention_mask"].append(attention_mask)
                # all_encodings["start_positions"].append(cls_index)
                # all_encodings["end_positions"].append(cls_index)
                # total_features += 1
                continue

            # å¦å‰‡ï¼ŒæŠŠ char å°é½Šåˆ° token index
            token_start = None
            token_end = None

            for idx in range(context_start, context_end + 1):
                if sequence_ids[idx] != 1:
                    continue
                if offsets[idx] is None:
                    continue
                s, e = offsets[idx]
                if token_start is None and s <= answer_start < e:
                    token_start = idx
                if s < answer_end <= e:
                    token_end = idx
                    break

            # æ‰¾ä¸åˆ°å°±é€€ CLSï¼ˆé¿å…çˆ†ï¼‰
            if token_start is None or token_end is None or token_end < token_start:
                token_start = cls_index
                token_end = cls_index

            all_encodings["input_ids"].append(input_ids)
            all_encodings["attention_mask"].append(attention_mask)
            all_encodings["start_positions"].append(token_start)
            all_encodings["end_positions"].append(token_end)
            total_features += 1

    print(f"âœ“ Prepared {total_features} features from {len(examples)} examples, skipped {skipped} bad examples")
    return all_encodings


class QADataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}


def train(train_path, context_path, model_dir):
    print("Loading tokenizer and model...")
    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)
    model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)

    print("Loading data...")
    data = load_data(train_path)
    contexts = load_context(context_path)

    # åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›† (90% train, 10% val)
    split_idx = int(len(data) * 0.9)
    train_data = data[:split_idx]
    eval_data = data[split_idx:]
    
    print(f"Loaded {len(train_data)} training examples and {len(eval_data)} validation examples")
    
    print("Preparing training features...")
    train_encodings = prepare_train_features(train_data, contexts, tokenizer)
    train_dataset = QADataset(train_encodings)
    
    print("Preparing validation features...")
    eval_encodings = prepare_train_features(eval_data, contexts, tokenizer)
    eval_dataset = QADataset(eval_encodings)

    # ä¿å­˜é©—è­‰é›†æ•¸æ“šï¼ˆè¨“ç·´å¾Œè©•ä¼°ç”¨ï¼‰
    print("Saving validation data...")
    with open(f"{model_dir}/val_data.json", "w", encoding="utf-8") as f:
        json.dump(eval_data, f, ensure_ascii=False, indent=2)

    print("Starting training...")
    args = TrainingArguments(
        output_dir=model_dir,
        per_device_train_batch_size=4,
        num_train_epochs=2,
        learning_rate=2e-5,
        logging_steps=100,
        save_steps=500,
        save_total_limit=10,
        warmup_steps=500,
        weight_decay=0.01,
        fp16=True,
        # åŠ é€™ä¸‰è¡Œ
        evaluation_strategy="steps",  # æˆ– "epoch"
        eval_steps=500,  # æ¯ 500 steps è©•ä¼°ä¸€æ¬¡
        load_best_model_at_end=True,  # è¨“ç·´çµæŸè¼‰å…¥æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_loss",
        greater_is_better=False,
    )

    trainer = Trainer(
        model=model, 
        args=args, 
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=6)]
    )

    trainer.train()

    print("Saving model...")
    trainer.save_model(model_dir)   # âœ… é€™å€‹æœƒå­˜ trainer.modelï¼ˆbestï¼‰
    tokenizer.save_pretrained(model_dir)
    
    # è¨“ç·´å®Œæˆå¾Œï¼Œå°é©—è­‰é›†é€²è¡Œå®Œæ•´è©•ä¼°
    print("\n" + "="*60)
    print("Evaluating on validation set...")
    print("="*60)
    
    val_pred_path = f"{model_dir}/val_predictions.json"
    predict_for_validation(model_dir, context_path, eval_data, contexts, val_pred_path)
    
    # è¨ˆç®— EM å’Œ F1
    print("\nComputing metrics...")
    predictions = load_data(val_pred_path)
    
    em_scores = []
    f1_scores = []
    
    for example in eval_data:
        qid = example["id"]
        true_answer = example["answers"][0]["text"]
        pred_answer = predictions.get(qid, "")
        
        em_scores.append(compute_em(pred_answer, true_answer))
        f1_scores.append(compute_f1(pred_answer, true_answer))
    
    val_em = np.mean(em_scores)
    val_f1 = np.mean(f1_scores)
    
    print("\n" + "="*60)
    print(f"ğŸ“Š Validation Results:")
    print(f"   EM:  {val_em:.4f} ({val_em*100:.2f}%)")
    print(f"   F1:  {val_f1:.4f} ({val_f1*100:.2f}%)")
    print("="*60)
    
    # ä¿å­˜è©•ä¼°çµæœ
    with open(f"{model_dir}/val_results.json", "w", encoding="utf-8") as f:
        json.dump({"em": val_em, "f1": val_f1, "count": len(eval_data)}, f, indent=2)
    
    print("âœ“ Training complete!")


def predict_for_validation(model_dir, context_path, eval_data, contexts, pred_path, max_length=512, doc_stride=128):
    """å°ˆé–€ç”¨æ–¼é©—è­‰é›†çš„é æ¸¬ï¼ˆeval_data å·²ç¶“æ˜¯åˆ—è¡¨æ ¼å¼ï¼‰"""
    print("Loading model for validation...")
    tokenizer = BertTokenizerFast.from_pretrained(model_dir)
    model = BertForQuestionAnswering.from_pretrained(model_dir)
    model.eval()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    print(f"Predicting {len(eval_data)} validation examples...")
    predictions = {}
    
    max_answer_length = 50
    
    for i, qa in enumerate(eval_data):
        if (i + 1) % 100 == 0:
            print(f"Progress: {i + 1}/{len(eval_data)}")
        
        qid = qa["id"]
        question = qa["question"]
        relevant_id = qa["relevant"]
        context = contexts[relevant_id]
        
        # â­ tokenize windows
        tokenized = tokenizer(
            question,
            context,
            truncation="only_second",
            max_length=max_length,
            stride=doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=False,
            padding="max_length",
            return_tensors="pt",
        )

        best_score = float("-inf")
        best_answer = ""

        # æ¯å€‹ window è·‘ä¸€æ¬¡
        for w in range(tokenized["input_ids"].size(0)):
            inputs = {k: v[w:w+1].to(device) for k, v in tokenized.items() if k in ["input_ids", "attention_mask", "token_type_ids"]}

            with torch.no_grad():
                outputs = model(**inputs)

            start_scores = outputs.start_logits[0]
            end_scores = outputs.end_logits[0]
            n = start_scores.size(0)

            # åªå…è¨± context token
            token_type_ids = inputs.get("token_type_ids", None)
            if token_type_ids is not None:
                tt = token_type_ids[0]
                start_scores = start_scores.masked_fill(tt != 1, float("-inf"))
                end_scores = end_scores.masked_fill(tt != 1, float("-inf"))

            score_matrix = start_scores[:, None] + end_scores[None, :]

            mask_triu = torch.triu(torch.ones(n, n, dtype=torch.bool, device=score_matrix.device))
            idx = torch.arange(n, device=score_matrix.device)
            mask_len = (idx[None, :] - idx[:, None]) < max_answer_length
            mask = mask_triu & mask_len
            score_matrix = score_matrix.masked_fill(~mask, float("-inf"))

            flat_best = score_matrix.argmax()
            s = (flat_best // n).item()
            e = (flat_best % n).item()
            score = score_matrix.view(-1)[flat_best].item()

            if score > best_score:
                best_score = score
                answer_ids = inputs["input_ids"][0][s:e+1]
                best_answer = tokenizer.decode(answer_ids, skip_special_tokens=True).replace(" ", "")

        predictions[qid] = best_answer if best_answer else "ç„¡ç­”æ¡ˆ"

    with open(pred_path, "w", encoding="utf-8") as f:
        json.dump(predictions, f, ensure_ascii=False, indent=2)

    print(f"âœ“ Validation predictions saved to {pred_path}")


def compute_em(pred, truth):
    """è¨ˆç®— Exact Match"""
    return int(pred == truth)


def compute_f1(pred, truth):
    """è¨ˆç®— F1 Score"""
    pred_tokens = list(pred)
    truth_tokens = list(truth)
    
    common = collections.Counter(pred_tokens) & collections.Counter(truth_tokens)
    num_same = sum(common.values())
    
    if num_same == 0:
        return 0
    
    precision = num_same / len(pred_tokens)
    recall = num_same / len(truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1


def predict(model_dir, context_path, data_path, pred_path, max_length=512, doc_stride=128):
    """é æ¸¬éšæ®µï¼šå°æ¯å€‹å•é¡Œçš„æ‰€æœ‰æ®µè½è©•åˆ†ï¼Œé¸æœ€é«˜çš„"""
    print("Loading model...")
    tokenizer = BertTokenizerFast.from_pretrained(model_dir)
    model = BertForQuestionAnswering.from_pretrained(model_dir)
    model.eval()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"Using device: {device}")
    
    print("Loading data...")
    data = load_data(data_path)
    contexts = load_context(context_path)
    
    print(f"Predicting {len(data)} examples...")
    predictions = {}
    
    max_answer_length = 50

    for i, qa in enumerate(data):
        if (i + 1) % 100 == 0:
            print(f"Progress: {i + 1}/{len(data)}")

        qid = qa["id"]
        question = qa["question"]
        paragraph_ids = qa["paragraphs"]

        best_answer = ""
        best_score = float("-inf")

        for pid in paragraph_ids:
            if pid >= len(contexts):
                continue

            context = contexts[pid]

            tokenized = tokenizer(
                question,
                context,
                truncation="only_second",
                max_length=max_length,
                stride=doc_stride,
                return_overflowing_tokens=True,
                return_offsets_mapping=False,
                padding="max_length",
                return_tensors="pt",
            )

            for w in range(tokenized["input_ids"].size(0)):
                inputs = {k: v[w:w+1].to(device) for k, v in tokenized.items() if k in ["input_ids", "attention_mask", "token_type_ids"]}

                with torch.no_grad():
                    outputs = model(**inputs)

                start_scores = outputs.start_logits[0]
                end_scores = outputs.end_logits[0]
                n = start_scores.size(0)

                token_type_ids = inputs.get("token_type_ids", None)
                if token_type_ids is not None:
                    tt = token_type_ids[0]
                    start_scores = start_scores.masked_fill(tt != 1, float("-inf"))
                    end_scores = end_scores.masked_fill(tt != 1, float("-inf"))

                score_matrix = start_scores[:, None] + end_scores[None, :]

                mask_triu = torch.triu(torch.ones(n, n, dtype=torch.bool, device=score_matrix.device))
                idx = torch.arange(n, device=score_matrix.device)
                mask_len = (idx[None, :] - idx[:, None]) < max_answer_length
                mask = mask_triu & mask_len
                score_matrix = score_matrix.masked_fill(~mask, float("-inf"))

                flat_best = score_matrix.argmax()
                s = (flat_best // n).item()
                e = (flat_best % n).item()
                score = score_matrix.view(-1)[flat_best].item()

                if score > best_score:
                    best_score = score
                    answer_ids = inputs["input_ids"][0][s:e+1]
                    best_answer = tokenizer.decode(answer_ids, skip_special_tokens=True).replace(" ", "")

        if not best_answer.strip():
            best_answer = "ç„¡ç­”æ¡ˆ"

        predictions[qid] = best_answer

    # â­ ä¿è­‰è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨ï¼ˆä½ ä¹‹å‰å¡åœ¨é€™ï¼‰
    out_dir = Path(pred_path).parent
    out_dir.mkdir(parents=True, exist_ok=True)

    print("Saving predictions...")
    with open(pred_path, "w", encoding="utf-8") as f:
        json.dump(predictions, f, ensure_ascii=False, indent=2)

    print(f"âœ“ Predictions saved to {pred_path}")


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage:")
        print("  Train:   python train.py train <train.json> <context.json> <model_dir>")
        print("  Predict: python train.py predict <model_dir> <context.json> <test.json> <output.json>")
        sys.exit(1)

    mode = sys.argv[1]

    if mode == "train":
        if len(sys.argv) != 5:
            print("Usage: python train.py train <train.json> <context.json> <model_dir>")
            sys.exit(1)
        train(sys.argv[2], sys.argv[3], sys.argv[4])
    elif mode == "predict":
        if len(sys.argv) != 6:
            print("Usage: python train.py predict <model_dir> <context.json> <test.json> <output.json>")
            sys.exit(1)
        predict(sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])
    else:
        print(f"Unknown mode: {mode}")
        print("Use 'train' or 'predict'")
        sys.exit(1)
